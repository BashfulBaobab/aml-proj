{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = tf.constant('Hello, TensorFlow')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "sess.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Sequence-to-sequence model with an attention mechanism.\"\"\"\n",
    "# # see https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html\n",
    "# # compare https://github.com/tflearn/tflearn/blob/master/examples/nlp/seq2seq_example.py\n",
    "# from __future__ import print_function\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# vocab_size=256 # We are lazy, so we avoid fency mapping and just use one *class* per character/byte\n",
    "# target_vocab_size=vocab_size\n",
    "# learning_rate=0.1\n",
    "# buckets=[(10, 10)] # our input and response words can be up to 10 characters long\n",
    "# PAD=[0] # fill words shorter than 10 characters with 'padding' zeroes\n",
    "# batch_size=10 # for parallel training (later)\n",
    "\n",
    "# input_data    = [map(ord, \"hello\") + PAD * 5] * batch_size\n",
    "# target_data   = [map(ord, \"world\") + PAD * 5] * batch_size\n",
    "# target_weights= [[1.0]*6 + [0.0]*4] *batch_size # mask padding. todo: redundant --\n",
    "\n",
    "# # EOS='\\n' # end of sequence symbol todo use how?\n",
    "# # GO=1\t\t # start symbol 0x01 todo use how?\n",
    "\n",
    "\n",
    "# class BabySeq2Seq(object):\n",
    "\n",
    "# \tdef __init__(self, source_vocab_size, target_vocab_size, buckets, size, num_layers, batch_size):\n",
    "# \t\tself.buckets = buckets\n",
    "# \t\tself.batch_size = batch_size\n",
    "# \t\tself.source_vocab_size = source_vocab_size\n",
    "# \t\tself.target_vocab_size = target_vocab_size\n",
    "\n",
    "# \t\tcell = single_cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "# \t\tif num_layers > 1:\n",
    "# \t\t cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "\n",
    "# \t\t# The seq2seq function: we use embedding for the input and attention.\n",
    "# \t\tdef seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "# \t\t\treturn tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "# \t\t\t\t\tencoder_inputs, decoder_inputs, cell,\n",
    "# \t\t\t\t\tnum_encoder_symbols=source_vocab_size,\n",
    "# \t\t\t\t\tnum_decoder_symbols=target_vocab_size,\n",
    "# \t\t\t\t\tembedding_size=size,\n",
    "# \t\t\t\t\tfeed_previous=do_decode)\n",
    "\n",
    "# \t\t# Feeds for inputs.\n",
    "# \t\tself.encoder_inputs = []\n",
    "# \t\tself.decoder_inputs = []\n",
    "# \t\tself.target_weights = []\n",
    "# \t\tfor i in xrange(buckets[-1][0]):\t# Last bucket is the biggest one.\n",
    "# \t\t\tself.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"encoder{0}\".format(i)))\n",
    "# \t\tfor i in xrange(buckets[-1][1] + 1):\n",
    "# \t\t\tself.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"decoder{0}\".format(i)))\n",
    "# \t\t\tself.target_weights.append(tf.placeholder(tf.float32, shape=[None], name=\"weight{0}\".format(i)))\n",
    "\n",
    "# \t\t# Our targets are decoder inputs shifted by one. OK\n",
    "# \t\ttargets = [self.decoder_inputs[i + 1] for i in xrange(len(self.decoder_inputs) - 1)]\n",
    "# \t\tself.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n",
    "# \t\t\t\tself.encoder_inputs, self.decoder_inputs, targets,\n",
    "# \t\t\t\tself.target_weights, buckets,\n",
    "# \t\t\t\tlambda x, y: seq2seq_f(x, y, False))\n",
    "\n",
    "# \t\t# Gradients update operation for training the model.\n",
    "# \t\tparams = tf.trainable_variables()\n",
    "# \t\tself.updates=[]\n",
    "# \t\tfor b in xrange(len(buckets)):\n",
    "# \t\t\tself.updates.append(tf.train.AdamOptimizer(learning_rate).minimize(self.losses[b]))\n",
    "\n",
    "# \t\tself.saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "# \tdef step(self, session, encoder_inputs, decoder_inputs, target_weights, test):\n",
    "# \t\tbucket_id=0 # todo: auto-select\n",
    "# \t\tencoder_size, decoder_size = self.buckets[bucket_id]\n",
    "\n",
    "# \t\t# Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "# \t\tinput_feed = {}\n",
    "# \t\tfor l in xrange(encoder_size):\n",
    "# \t\t\tinput_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "# \t\tfor l in xrange(decoder_size):\n",
    "# \t\t\tinput_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "# \t\t\tinput_feed[self.target_weights[l].name] = target_weights[l]\n",
    "\n",
    "# \t\t# Since our targets are decoder inputs shifted by one, we need one more.\n",
    "# \t\tlast_target = self.decoder_inputs[decoder_size].name\n",
    "# \t\tinput_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "\n",
    "# \t\t# Output feed: depends on whether we do a backward step or not.\n",
    "# \t\tif not test:\n",
    "# \t\t\toutput_feed = [self.updates[bucket_id], self.losses[bucket_id]]\n",
    "# \t\telse:\n",
    "# \t\t\toutput_feed = [self.losses[bucket_id]]\t# Loss for this batch.\n",
    "# \t\t\tfor l in xrange(decoder_size):\t# Output logits.\n",
    "# \t\t\t\toutput_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "# \t\toutputs = session.run(output_feed, input_feed)\n",
    "# \t\tif not test:\n",
    "# \t\t\treturn outputs[0], outputs[1]# Gradient norm, loss\n",
    "# \t\telse:\n",
    "# \t\t\treturn outputs[0], outputs[1:]# loss, outputs.\n",
    "\n",
    "\n",
    "# def decode(bytes):\n",
    "# \treturn \"\".join(map(chr, bytes)).replace('\\x00', '').replace('\\n', '')\n",
    "\n",
    "# def test():\n",
    "# \tperplexity, outputs = model.step(session, input_data, target_data, target_weights, test=True)\n",
    "# \twords = np.argmax(outputs, axis=2)  # shape (10, 10, 256)\n",
    "# \tword = decode(words[0])\n",
    "# \tprint(\"step %d, perplexity %f, output: hello %s?\" % (step, perplexity, word))\n",
    "# \tif word == \"world\":\n",
    "# \t\tprint(\">>>>> success! hello \" + word + \"! <<<<<<<\")\n",
    "# \t\texit()\n",
    "\n",
    "# step=0\n",
    "# test_step=1\n",
    "# with tf.Session() as session:\n",
    "# \tmodel= BabySeq2Seq(vocab_size, target_vocab_size, buckets, size=10, num_layers=1, batch_size=batch_size)\n",
    "# \tsession.run(tf.initialize_all_variables())\n",
    "# \twhile True:\n",
    "# \t\tmodel.step(session, input_data, target_data, target_weights, test=False) # no outputs in training\n",
    "# \t\tif step % test_step == 0:\n",
    "# \t\t\ttest()\n",
    "# \t\tstep=step+1\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9de5165c937a2b35058362d46df331dd31555248ff77b80de32d4bef5809b318"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('aml-proj': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
